{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Aarish Alam","text":"<p>AI Engineer working on Building AI Agents and Focusing on Evals for AI Agents</p> <p>Currently building intelligent systems at Scaled Focus, where I focus on building AI agents and focusing on evals for AI agents. I'm currently working on Context Engineering for AI Agents. I've previously built tagging and search engine for Fold money featured here.</p>"},{"location":"#what-i-do","title":"What I Do","text":"<p>I specialize in building and scaling AI agents, particularly in the areas of:</p> <ul> <li>AI Agents for any and every thing!</li> <li>Evals For AI Workflows</li> <li>Infrastructure for Scaling AI Agents in Production</li> </ul>"},{"location":"#recent-highlights","title":"Recent Highlights","text":"<ul> <li>\ud83c\udfa4 Speaking: Presented at PyData Bangalore on \"Scalable Hybrid RAG\"</li> <li>\ud83d\udd27 Engineering: Scaled Search Systems featured here, built Voice and Chat Agents for fortune 500 companies</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>About - Learn more about my background and expertise</li> <li>Projects - Explore my technical work and AI solutions</li> <li>Experience - My professional journey in AI and engineering</li> <li>Speaking - Talks and presentations I've given</li> <li>Contact - Get in touch for collaborations or consulting</li> </ul> <p>Always excited to discuss AI, machine learning, and building systems that scale. Let's connect!</p>"},{"location":"about/","title":"About Me","text":"<p>I'm an AI Engineer passionate about building intelligent systems that scale. Currently working as an AI Consultant at Scaled Focus, Currently working on building AI agents and focusing on evals for AI agents.</p>"},{"location":"about/#background","title":"Background","text":"<p>My journey into AI began with a fascination for how machines can understand and process human language. Over the years, I've evolved from working on traditional machine learning models to building sophisticated AI systems that power real-world applications.</p>"},{"location":"about/#technical-expertise","title":"Technical Expertise","text":""},{"location":"about/#languages-framework","title":"Languages &amp; Framework","text":"<ul> <li>Datbases : Postgres , Redis , Qdrant</li> <li>Frameworks : Langchain , Langgraph , Dspy , MCP , RAGAS</li> </ul>"},{"location":"about/#cloud-infrastructure","title":"Cloud &amp; Infrastructure","text":"<ul> <li>Cloud Platforms: Azure Cloud, AWS</li> <li>DevOps: Git, Github Actions, Docker, Kubernetes , ECS </li> <li>Monitoring: Prometheus , Grafana , Phoenix , Langsmith</li> </ul>"},{"location":"about/#current-focus","title":"Current Focus","text":"<p>At Scaled Focus, I'm working on:</p> <ul> <li>AI Agents: Dspy, MCP, RAGAS, Langgraph, Langchain</li> <li>Evals: Langsmith, Prometheus, Grafana, Phoenix</li> <li>Infrastructure: Docker, Kubernetes, ECS</li> </ul>"},{"location":"about/#beyond-work","title":"Beyond Work","text":"<p>When I'm not coding, I enjoy:</p> <ul> <li>Speaking: Sharing knowledge at conferences and meetups</li> <li>Sports: Badminton , Cricket , Football</li> </ul> <p>Interested in collaborating or discussing AI? Feel free to reach out!</p>"},{"location":"contact/","title":"Get In Touch","text":"<p>I'm always excited to connect with fellow AI enthusiasts, potential collaborators, and anyone interested in building intelligent systems. Let's discuss how we can work together!</p>"},{"location":"contact/#how-to-reach-me","title":"How to Reach Me","text":""},{"location":"contact/#email","title":"Email","text":"<p>\ud83d\udce7 arishalam121@gmail.com</p> <p>Primary method for professional inquiries, collaboration opportunities, and detailed discussions.</p>"},{"location":"contact/#social-media","title":"Social Media","text":"<ul> <li>\ud83d\udc26 Twitter: @rheagalfire - AI insights and industry updates</li> <li>\ud83d\udcbc LinkedIn: Aarish Alam - Professional network and career updates</li> <li>\ud83d\udcf8 Instagram: @rheagalfire - Behind-the-scenes and personal updates</li> <li>\ud83d\udcbb GitHub: rheagalfire - Code, projects, and open-source contributions</li> </ul>"},{"location":"contact/#what-im-looking-for","title":"What I'm Looking For","text":""},{"location":"contact/#collaboration-opportunities","title":"Collaboration Opportunities","text":"<ul> <li>AI research projects can work across AI agents and evals</li> <li>Open-source contributions</li> <li>Technical writing and content creation</li> <li>Speaking engagements and workshops</li> </ul>"},{"location":"contact/#community-engagement","title":"Community Engagement","text":"<ul> <li>Conference speaking invitations</li> <li>Podcast appearances</li> <li>Workshop facilitation</li> <li>Technical advisory roles</li> </ul>"},{"location":"contact/#areas-of-expertise","title":"Areas of Expertise","text":"<p>Feel free to reach out if you'd like to discuss:</p>"},{"location":"contact/#technical-topics","title":"Technical Topics","text":"<ul> <li>AI Systems: Search Systems, Chat &amp; Voice Systems , AI Agents , RAG systems</li> <li>MLOps: Model deployment, Scaling and Monitoring in production</li> <li>Software Engineering: MLops , AIops</li> </ul>"},{"location":"contact/#response-time","title":"Response Time","text":"<p>I typically respond to emails within 24-48 hours. For urgent matters, feel free to mention it in your subject line.</p>"},{"location":"contact/#lets-connect","title":"Let's Connect!","text":"<p>Whether you're: - A fellow AI engineer looking to collaborate - A startup seeking technical guidance - A conference organizer looking for speakers - A student seeking mentorship - A company exploring AI solutions</p> <p>I'd love to hear from you and explore how we can work together to build amazing AI systems!</p> <p>Looking forward to connecting and discussing the exciting world of AI engineering with you!</p>"},{"location":"experience/","title":"Experience","text":"<p>My professional journey in AI and engineering, building scalable solutions across various industries.</p>"},{"location":"experience/#current-role","title":"Current Role","text":""},{"location":"experience/#scaled-focus-ai-consultant","title":"Scaled Focus | AI Consultant","text":"<p>April 2025 - Present</p> <p>Working on building AI agents and focusing on evals for AI agents.</p>"},{"location":"experience/#previous-experience","title":"Previous Experience","text":""},{"location":"experience/#futurepath-ai-ai-engineer","title":"Futurepath AI | AI Engineer","text":"<p>May 2024 - May 2025</p> <p>Built semantic chat and voice systems using agentic workflows, Hybrid RAG, and RRF\u2014achieving over 97% precision and context recall. Developed LLM observability pipelines with Phoenix and eval suites using RAGAS. Also built agentic systems for querying Kubernetes pods and databases with Langgraph and MCP.</p>"},{"location":"experience/#fold-ai-engineer","title":"Fold | AI Engineer","text":"<p>July 2023 - March 2024</p> <p>Fine-tuned DistilBERT to categorize transactions from narration notes (97% precision, 90% recall). Built a smart search model using RASA and semantic search over vector databases for intent-based filtering and SQL conversion. Developed a hybrid tagging system combining rule-based and ML approaches to personalize tagging by learning user and merchant semantics.</p>"},{"location":"experience/#phonepe-automation-engineer","title":"PhonePe | Automation Engineer","text":"<p>January 2023 - June 2023</p> <p>Developed an ETL pipeline to process agent training documents and performance metrics, automating data extraction, transformation, and loading into a centralized warehouse. Built dynamic dashboards in Looker Studio for real-time monitoring and analysis of agent performance. Optimized support workflows by applying K-means clustering to support tickets and embedding multilingual call transcripts with the Jina-Embeddings-v3 model, enabling effective clustering and identification of solutions and reference calls.</p>"},{"location":"experience/#shopagain-data-scientist","title":"Shopagain | Data Scientist","text":"<p>December 2021 - December 2022</p> <p>Developed machine learning modules for e-commerce, including customer lifetime value prediction using browsing, purchase, and demographic data. Built and deployed a real-time intent prediction system to assess customer purchase intent from session data, complemented by comprehensive monitoring for production reliability. Also contributed to recommendation systems and customer behavior analytics to drive business insights.</p>"},{"location":"experience/#unmazerai-data-scientist","title":"Unmazer.ai | Data Scientist","text":"<p>May 2021 - August 2021</p> <p>Developed REST APIs with database integration and CI/CD, deployed on AWS. Built data pipelines using AWS services, implemented neighbourhood analysis with DBSCAN and HDBSCAN, and leveraged Uber H3 for spatial indexing. Worked with alternate data sources for hyperlocal intelligence.</p> <p>Looking for opportunities to collaborate or discuss AI engineering? Get in touch!</p>"},{"location":"projects/","title":"Projects","text":"<p>Here are some of the key projects I've worked on, showcasing my expertise in AI, machine learning, and software engineering.</p>"},{"location":"projects/#featured-projects","title":"Featured Projects","text":""},{"location":"projects/#askdoc","title":"\ud83e\ude7a AskDoc","text":"<p>Symptom-based Disease Recommendation System</p> <p>An intelligent health assistant that analyzes symptoms and provides preliminary disease recommendations using machine learning algorithms.</p> <p>Tech Stack: Python, TensorFlow, NLP, Medical Data APIs Impact: Helps users get initial health insights before consulting healthcare professionals</p>"},{"location":"projects/#emosong","title":"\ud83c\udfb5 Emosong","text":"<p>Emotion-based Music Recommendation</p> <p>A music recommendation system that analyzes user emotions and suggests songs that match their current mood using sentiment analysis and collaborative filtering.</p> <p>Tech Stack: Python, PyTorch, Spotify API, Sentiment Analysis Features: Real-time emotion detection, personalized playlists, mood tracking</p>"},{"location":"projects/#llm-gym","title":"\ud83e\udd16 LLM-Gym","text":"<p>Search and Chat Tool</p> <p>A comprehensive platform for testing and comparing different large language models in search and conversational contexts.</p> <p>Tech Stack: Python, Langchain, Multiple LLM APIs, Vector Databases Purpose: Benchmarking LLM performance for various use cases</p>"},{"location":"projects/#malaria-detection","title":"\ud83e\udd9f Malaria Detection","text":"<p>Transfer Learning for Medical Diagnosis</p> <p>A computer vision system that detects malaria parasites in blood cell images using transfer learning techniques.</p> <p>Tech Stack: Python, TensorFlow, Computer Vision, Transfer Learning Achievement: High accuracy in parasite detection, potential for field deployment</p>"},{"location":"projects/#sign-language-detection","title":"\ud83e\udd1f Sign Language Detection","text":"<p>Real-time Sign Language Recognition</p> <p>A computer vision system that recognizes sign language gestures in real-time, making communication more accessible.</p> <p>Tech Stack: Python, OpenCV, Deep Learning, Real-time Processing Impact: Improving accessibility for deaf and hard-of-hearing communities</p>"},{"location":"projects/#production-systems","title":"Production Systems","text":""},{"location":"projects/#voice-assistant-platform","title":"\ud83d\udde3\ufe0f Voice Assistant Platform","text":"<p>Enterprise Voice AI Solution</p> <p>Built high-precision voice assistants for enterprise clients with 97% accuracy in intent recognition.</p> <p>Key Features: - Real-time speech processing - Custom wake word detection - Multi-language support - Integration with business systems</p>"},{"location":"projects/#transaction-tagging-engine","title":"\ud83d\udcb3 Transaction Tagging Engine","text":"<p>Financial Data Classification</p> <p>Developed an AI system for automatically categorizing financial transactions with 95% F1 score.</p> <p>Technical Highlights: - Natural language processing for transaction descriptions - Machine learning classification models - Real-time processing pipeline - Integration with banking systems</p>"},{"location":"projects/#open-source-contributions","title":"Open Source Contributions","text":"<p>I actively contribute to the AI community through open-source projects and tools that help other developers build better AI systems.</p>"},{"location":"projects/#areas-of-contribution","title":"Areas of Contribution","text":"<ul> <li>RAG Systems: Tools for retrieval-augmented generation</li> <li>LLM Utilities: Helper libraries for working with language models</li> <li>AI Infrastructure: Deployment and monitoring tools</li> <li>Educational Resources: Tutorials and documentation</li> </ul>"},{"location":"projects/#future-projects","title":"Future Projects","text":"<p>I'm constantly exploring new ideas and technologies. Some areas I'm excited to work on include:</p> <ul> <li>Multimodal AI: Combining text, vision, and audio processing</li> <li>Edge AI: Deploying AI models on resource-constrained devices</li> <li>AI Safety: Building more reliable and safe AI systems</li> <li>Conversational AI: Next-generation chatbots and virtual assistants</li> </ul> <p>Interested in collaborating on any of these projects? Let's connect!</p>"},{"location":"speaking/","title":"Speaking &amp; Presentations","text":"<p>I'm passionate about sharing knowledge and experiences with the AI community through conferences, meetups, and workshops.</p>"},{"location":"speaking/#featured-talk","title":"Featured Talk","text":""},{"location":"speaking/#pydata-bangalore-scalable-hybrid-rag","title":"PyData Bangalore | \"Scalable Hybrid RAG\"","text":"<p>Recent Presentation</p> <p>Presented insights on building scalable Retrieval-Augmented Generation (RAG) systems that combine the best of both retrieval and generation approaches.</p> <p>GitHub Repository: Scalable Hybrid RAG</p> <p>Talk Overview: - Challenges in scaling RAG systems for production - Hybrid approaches combining dense and sparse retrieval - Practical implementation strategies - Performance optimization techniques</p> <p>Key Takeaways: - How to design RAG systems that scale with increasing data volumes - Balancing accuracy and performance in production environments - Best practices for vector database optimization</p>"},{"location":"speaking/#book-a-speaking-engagement","title":"Book a Speaking Engagement","text":"<p>I'm available for:</p> <ul> <li>Conference Keynotes: Industry trends and future of AI</li> <li>Technical Workshops: Hands-on learning experiences</li> <li>Corporate Training: Team development and upskilling</li> <li>Panel Discussions: Industry insights and perspectives</li> <li>Podcast Interviews: Deep dives into AI topics</li> </ul>"},{"location":"speaking/#what-i-bring","title":"What I Bring","text":"<ul> <li>Deep technical expertise in AI and engineering</li> <li>Real-world production experience</li> <li>Engaging presentation style</li> <li>Practical, actionable insights</li> <li>Passion for knowledge sharing</li> </ul> <p>Interested in having me speak at your event or on your podcast? Let's discuss!</p>"},{"location":"writing/","title":"Writing","text":"<p>Welcome to my writing section. Here you'll find my thoughts on AI engineering, search technologies, and my experiences in the field.</p>"},{"location":"writing/#latest-articles","title":"Latest Articles","text":""},{"location":"writing/#building-scalable-search-systems-with-vector-databases","title":"Building Scalable Search Systems with Vector Databases","text":"<p>December 2024 \u2022 8 min read</p> <p>Exploring the architecture patterns and best practices for building search systems that can scale to millions of users while maintaining sub-second response times.</p>"},{"location":"writing/#building-intelligent-search-systems-with-llm-gym-architecture","title":"Building Intelligent Search Systems with LLM-Gym Architecture","text":"<p>November 2024 \u2022 6 min read</p> <p>Exploring the architecture patterns and implementation strategies for building sophisticated search and chat systems over curated content.</p>"},{"location":"writing/#scaling-transaction-tagging-from-rules-to-ml-at-10m-transactions","title":"Scaling Transaction Tagging: From Rules to ML at 10M+ Transactions","text":"<p>October 2024 \u2022 10 min read</p> <p>Building intelligent financial systems that process over 10 million transactions using a three-stage ML pipeline: lexical rules, F1 tagging, and DistilBERT categorization.</p>"},{"location":"writing/#categories","title":"Categories","text":""},{"location":"writing/#ai-engineering","title":"AI Engineering","text":"<p>Technical deep-dives into AI system architecture and implementation</p>"},{"location":"writing/#search-retrieval","title":"Search &amp; Retrieval","text":"<p>Articles about search technologies, vector databases, and information retrieval</p>"},{"location":"writing/#enterprise-applications","title":"Enterprise Applications","text":"<p>Insights about implementing AI in business environments</p>"},{"location":"writing/llm-gym-architecture/","title":"Building Intelligent Search Systems with LLM-Gym Architecture","text":"<p>Published: November 2024 \u2022 6 min read</p>"},{"location":"writing/llm-gym-architecture/#introduction","title":"Introduction","text":"<p>Modern knowledge management requires more than traditional search - it needs intelligent systems that can understand context, semantics, and user intent. This article explores the architecture of LLM-Gym, a personal project demonstrating how to build sophisticated search and chat systems over curated content.</p>"},{"location":"writing/llm-gym-architecture/#system-overview","title":"System Overview","text":"<p>LLM-Gym implements a three-layer orchestration pattern for intelligent content processing:</p> <pre><code>graph TD\n    A[GitHub Links] --&gt; B[Data Processing Layer]\n    B --&gt; C[Indexing Layer]\n    C --&gt; D[App Engine]\n    D --&gt; E[Search Interface]\n    D --&gt; F[Chat Interface]\n\n    subgraph \"Data Processing Layer\"\n        B1[Webhook Handler] --&gt; B2[Content Scraper]\n        B2 --&gt; B3[Data Storage]\n    end\n\n    subgraph \"Indexing Layer\"\n        C1[Text Processing] --&gt; C2[Vector Embeddings]\n        C2 --&gt; C3[Full-text Indexing]\n    end\n\n    subgraph \"App Engine\"\n        D1[Query Router] --&gt; D2[Hybrid Search]\n        D2 --&gt; D3[Response Generator]\n    end</code></pre>"},{"location":"writing/llm-gym-architecture/#architecture-patterns","title":"Architecture Patterns","text":""},{"location":"writing/llm-gym-architecture/#hybrid-search-strategy","title":"Hybrid Search Strategy","text":"<p>The system combines two complementary search approaches:</p> <pre><code>graph LR\n    A[User Query] --&gt; B[Query Processing]\n    B --&gt; C[Meilisearch BM25]\n    B --&gt; D[Qdrant Vector Search]\n    C --&gt; E[Result Fusion]\n    D --&gt; E\n    E --&gt; F[Ranked Results]</code></pre> <p>Benefits of Hybrid Approach: - BM25 (Meilisearch): Excellent for exact keyword matching - Vector Search (Qdrant): Captures semantic similarity - Combined Results: Best of both worlds</p>"},{"location":"writing/llm-gym-architecture/#multi-database-architecture","title":"Multi-Database Architecture","text":"<pre><code># Database specialization\ndatabases = {\n    \"postgres\": \"Structured data, relationships\",\n    \"qdrant\": \"Vector embeddings, semantic search\",\n    \"meilisearch\": \"Full-text search, faceted search\"\n}\n</code></pre> <p>Each database serves its optimal use case:</p> <pre><code>graph TB\n    A[Application Layer] --&gt; B[Postgres]\n    A --&gt; C[Qdrant]\n    A --&gt; D[Meilisearch]\n\n    B --&gt; B1[User Data&lt;br/&gt;Metadata&lt;br/&gt;Relationships]\n    C --&gt; C1[Vector Embeddings&lt;br/&gt;Semantic Search&lt;br/&gt;Similarity Queries]\n    D --&gt; D1[Full-text Search&lt;br/&gt;Faceted Search&lt;br/&gt;Exact Matching]</code></pre>"},{"location":"writing/llm-gym-architecture/#core-components","title":"Core Components","text":""},{"location":"writing/llm-gym-architecture/#data-processing-layer","title":"Data Processing Layer","text":"<p>Handles incoming data with automated workflows:</p> <pre><code>class DataProcessor:\n    def process_github_webhook(self, payload):\n        # Extract content from GitHub links\n        content = self.scrape_content(payload.url)\n\n        # Store structured data\n        self.store_metadata(content)\n\n        # Queue for indexing\n        self.queue_for_indexing(content)\n</code></pre>"},{"location":"writing/llm-gym-architecture/#indexing-layer","title":"Indexing Layer","text":"<p>Transforms raw content into searchable formats:</p> <pre><code>sequenceDiagram\n    participant C as Content\n    participant P as Processor\n    participant V as Vector DB\n    participant F as Full-text DB\n\n    C-&gt;&gt;P: Raw Content\n    P-&gt;&gt;P: Text Cleaning\n    P-&gt;&gt;P: Chunk Generation\n    P-&gt;&gt;V: Store Embeddings\n    P-&gt;&gt;F: Store Text Index\n    V--&gt;&gt;P: Success\n    F--&gt;&gt;P: Success</code></pre>"},{"location":"writing/llm-gym-architecture/#app-engine","title":"App Engine","text":"<p>Orchestrates search and chat interactions:</p> <pre><code>class SearchEngine:\n    def hybrid_search(self, query, k=10):\n        # Get results from both engines\n        vector_results = self.qdrant_search(query, k)\n        text_results = self.meilisearch_search(query, k)\n\n        # Fusion strategy\n        return self.reciprocal_rank_fusion(\n            vector_results, \n            text_results\n        )\n</code></pre>"},{"location":"writing/llm-gym-architecture/#implementation-highlights","title":"Implementation Highlights","text":""},{"location":"writing/llm-gym-architecture/#containerized-development","title":"Containerized Development","text":"<pre><code># docker-compose.yml structure\nservices:\n  app:\n    build: .\n    depends_on: [postgres, qdrant, meilisearch]\n\n  postgres:\n    image: postgres:15\n\n  qdrant:\n    image: qdrant/qdrant\n\n  meilisearch:\n    image: getmeili/meilisearch\n</code></pre>"},{"location":"writing/llm-gym-architecture/#modern-python-stack","title":"Modern Python Stack","text":"<pre><code># Key dependencies\ndependencies = [\n    \"dspy-ai\",          # LLM framework\n    \"instructor\",       # Structured outputs\n    \"prisma\",          # Database ORM\n    \"qdrant-client\",   # Vector database\n    \"meilisearch\",     # Search engine\n]\n</code></pre>"},{"location":"writing/llm-gym-architecture/#semantic-search-implementation","title":"Semantic Search Implementation","text":""},{"location":"writing/llm-gym-architecture/#embedding-strategy","title":"Embedding Strategy","text":"<pre><code>def create_embeddings(content):\n    # Chunk content appropriately\n    chunks = self.chunk_content(content)\n\n    # Generate embeddings\n    embeddings = []\n    for chunk in chunks:\n        embedding = self.embedding_model.encode(chunk)\n        embeddings.append({\n            \"vector\": embedding,\n            \"metadata\": {\n                \"content\": chunk,\n                \"source\": content.url\n            }\n        })\n\n    return embeddings\n</code></pre>"},{"location":"writing/llm-gym-architecture/#query-processing","title":"Query Processing","text":"<pre><code>graph TD\n    A[User Query] --&gt; B[Query Analysis]\n    B --&gt; C{Query Type}\n    C --&gt;|Factual| D[Direct Search]\n    C --&gt;|Conversational| E[Context Building]\n    C --&gt;|Exploratory| F[Semantic Search]\n\n    D --&gt; G[Meilisearch]\n    E --&gt; H[Vector Search + LLM]\n    F --&gt; I[Hybrid Search]</code></pre>"},{"location":"writing/llm-gym-architecture/#chat-interface-integration","title":"Chat Interface Integration","text":""},{"location":"writing/llm-gym-architecture/#context-aware-responses","title":"Context-Aware Responses","text":"<pre><code>class ChatHandler:\n    def generate_response(self, query, search_results):\n        # Build context from search results\n        context = self.build_context(search_results)\n\n        # Generate response with LLM\n        response = self.llm.generate(\n            query=query,\n            context=context,\n            max_tokens=500\n        )\n\n        return response\n</code></pre>"},{"location":"writing/llm-gym-architecture/#conversation-flow","title":"Conversation Flow","text":"<pre><code>graph LR\n    A[User Question] --&gt; B[Search Content]\n    B --&gt; C[Build Context]\n    C --&gt; D[Generate Response]\n    D --&gt; E[Return Answer + Sources]\n\n    subgraph \"Context Building\"\n        C1[Relevant Documents] --&gt; C2[Summarization]\n        C2 --&gt; C3[Context Window]\n    end</code></pre>"},{"location":"writing/llm-gym-architecture/#scaling-considerations","title":"Scaling Considerations","text":""},{"location":"writing/llm-gym-architecture/#performance-optimization","title":"Performance Optimization","text":"<ol> <li>Caching Strategy: Cache frequent queries and embeddings</li> <li>Batch Processing: Process multiple documents efficiently</li> <li>Async Operations: Non-blocking I/O for web scraping</li> </ol>"},{"location":"writing/llm-gym-architecture/#resource-management","title":"Resource Management","text":""},{"location":"writing/llm-gym-architecture/#configuration-for-different-environments-config-development-embedding_batch_size-10-max_concurrent_requests-5-production-embedding_batch_size-100-max_concurrent_requests-50","title":"<pre><code># Configuration for different environments\nconfig = {\n    \"development\": {\n        \"embedding_batch_size\": 10,\n        \"max_concurrent_requests\": 5\n    },\n    \"production\": {\n        \"embedding_batch_size\": 100,\n        \"max_concurrent_requests\": 50\n    }\n}\n</code></pre>","text":"<p>This article is part of my ongoing series on AI engineering. Check out the writing section for more articles.</p>"},{"location":"writing/scalable-search-systems/","title":"Building Scalable Search Systems with Vector Databases","text":"<p>Published: March 2025 \u2022 8 min read</p>"},{"location":"writing/scalable-search-systems/#introduction","title":"Introduction","text":"<p>Modern AI applications demand search systems that can handle millions of users while maintaining sub-second response times. This article explores the architecture patterns and implementation strategies for building scalable hybrid search systems, drawing from real-world experience with code search applications.</p>"},{"location":"writing/scalable-search-systems/#system-architecture","title":"System Architecture","text":""},{"location":"writing/scalable-search-systems/#core-components","title":"Core Components","text":"<p>A scalable search system consists of several key components working together:</p> <ul> <li>Vector Database: Specialized storage for high-dimensional embeddings</li> <li>Embedding Models: Separate models for text and code domains</li> <li>Hybrid Search Engine: Combining dense and sparse retrieval methods</li> <li>Query Processing Pipeline: Handling multi-modal search requests</li> </ul>"},{"location":"writing/scalable-search-systems/#technology-stack","title":"Technology Stack","text":"<p>For production-ready systems, consider this proven stack:</p> <pre><code># Vector Database: Qdrant\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import VectorParams, Distance\n\n# Embedding Models\ntext_model = \"sentence-transformers/all-MiniLM-L6-v2\"  # 384 dimensions\ncode_model = \"jinaai/jina-embeddings-v2-base-code\"     # 768 dimensions\n</code></pre>"},{"location":"writing/scalable-search-systems/#hybrid-search-implementation","title":"Hybrid Search Implementation","text":""},{"location":"writing/scalable-search-systems/#dual-embedding-strategy","title":"Dual Embedding Strategy","text":"<p>The key to effective hybrid search is using specialized embeddings for different content types:</p> <pre><code>def create_hybrid_embeddings(text_content, code_content):\n    # Text embedding for natural language\n    text_embedding = text_model.encode(text_content)\n\n    # Code embedding for technical content\n    code_embedding = code_model.encode(code_content)\n\n    return text_embedding, code_embedding\n</code></pre>"},{"location":"writing/scalable-search-systems/#reciprocal-rank-fusion-rrf","title":"Reciprocal Rank Fusion (RRF)","text":"<p>Combine results from multiple search strategies using RRF:</p> <pre><code>def hybrid_search(query, k=10):\n    # Search across both embedding spaces\n    text_results = search_text_embeddings(query)\n    code_results = search_code_embeddings(query)\n\n    # Apply RRF to combine rankings\n    combined_results = reciprocal_rank_fusion(\n        [text_results, code_results], \n        k=k\n    )\n\n    return combined_results\n</code></pre>"},{"location":"writing/scalable-search-systems/#performance-optimization","title":"Performance Optimization","text":""},{"location":"writing/scalable-search-systems/#quantization-for-speed","title":"Quantization for Speed","text":"<p>Implementing scalar quantization can dramatically improve search performance:</p> <ul> <li>11x faster search times for code search tasks</li> <li>Reduced memory footprint with minimal accuracy loss</li> <li>On-disk storage with in-memory quantized vectors</li> </ul> <pre><code># Configure quantized collection\nvector_params = VectorParams(\n    size=768,\n    distance=Distance.COSINE,\n    quantization=ScalarQuantization(\n        type=ScalarType.INT8,\n        quantile=0.99,\n        always_ram=True\n    )\n)\n</code></pre>"},{"location":"writing/scalable-search-systems/#batch-processing","title":"Batch Processing","text":"<p>Optimize data ingestion with parallel processing:</p> <pre><code>def batch_embed_documents(documents, batch_size=100):\n    with ThreadPoolExecutor(max_workers=4) as executor:\n        futures = []\n        for batch in chunks(documents, batch_size):\n            future = executor.submit(embed_batch, batch)\n            futures.append(future)\n\n        results = [future.result() for future in futures]\n    return results\n</code></pre>"},{"location":"writing/scalable-search-systems/#scaling-patterns","title":"Scaling Patterns","text":""},{"location":"writing/scalable-search-systems/#collection-strategy","title":"Collection Strategy","text":"<p>Separate collections by use case and performance requirements:</p> <ul> <li>Normal Collection: High accuracy, slower queries</li> <li>Quantized Collection: Fast queries, slight accuracy trade-off</li> <li>Specialized Collections: Domain-specific optimizations</li> </ul>"},{"location":"writing/scalable-search-systems/#memory-management","title":"Memory Management","text":"<p>Balance between speed and resource usage:</p> <pre><code># Configure memory-efficient settings\ncollection_config = {\n    \"vectors\": {\n        \"size\": 768,\n        \"distance\": \"Cosine\",\n        \"on_disk\": True,  # Store on disk\n        \"quantization\": {\n            \"scalar\": {\n                \"type\": \"int8\",\n                \"quantile\": 0.99,\n                \"always_ram\": True  # Keep quantized in memory\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"writing/scalable-search-systems/#data-preprocessing","title":"Data Preprocessing","text":""},{"location":"writing/scalable-search-systems/#chunking-strategy","title":"Chunking Strategy","text":"<p>Effective document chunking is crucial for search quality:</p> <pre><code>def preprocess_code_data(problem_description, code_solution):\n    # Combine problem context with solution\n    combined_text = f\"{problem_description}\\n\\nSolution:\\n{code_solution}\"\n\n    # Filter and clean data\n    if len(combined_text) &gt; 10000:  # Skip very long documents\n        return None\n\n    return {\n        \"text\": problem_description,\n        \"code\": code_solution,\n        \"combined\": combined_text\n    }\n</code></pre>"},{"location":"writing/scalable-search-systems/#evaluation-and-monitoring","title":"Evaluation and Monitoring","text":""},{"location":"writing/scalable-search-systems/#performance-metrics","title":"Performance Metrics","text":"<p>Track key metrics for production systems:</p> <ul> <li>Search Latency: Target sub-100ms response times</li> <li>Throughput: Queries per second under load</li> <li>Accuracy: Relevance of search results</li> <li>NDCG Score: Normalized Discounted Cumulative Gain</li> </ul>"},{"location":"writing/transaction-tagging-ml/","title":"Scaling Transaction Tagging: From Rules to ML at 10M+ Transactions","text":"<p>Published: December 2024 \u2022 10 min read</p>"},{"location":"writing/transaction-tagging-ml/#introduction","title":"Introduction","text":"<p>Building intelligent financial systems requires solving complex data challenges at scale. This article explores the architecture and implementation of an automated transaction tagging system that processes over 10 million transactions, transitioning from rule-based approaches to sophisticated ML pipelines.</p> <p>Based on work done at Fold - read the original blog post for additional context.</p>"},{"location":"writing/transaction-tagging-ml/#system-architecture","title":"System Architecture","text":"<p>The transaction tagging pipeline implements a three-stage approach designed for scalability and accuracy:</p> <pre><code>graph TD\n    A[Raw Transaction Data] --&gt; B[Lexical Rule-based System]\n    B --&gt; C[Intelligent F1 Tagging]\n    C --&gt; D[ML-based Categorization]\n    D --&gt; E[Tagged Transactions]\n\n    subgraph \"Stage 1: Preprocessing\"\n        B1[Text Normalization] --&gt; B2[Merchant Extraction]\n        B2 --&gt; B3[Rule Application]\n    end\n\n    subgraph \"Stage 2: Learning System\"\n        C1[User Behavior Analysis] --&gt; C2[Pattern Recognition]\n        C2 --&gt; C3[F1 Score Optimization]\n    end\n\n    subgraph \"Stage 3: ML Pipeline\"\n        D1[DistilBERT Embeddings] --&gt; D2[Vector Similarity]\n        D2 --&gt; D3[Category Prediction]\n    end</code></pre>"},{"location":"writing/transaction-tagging-ml/#stage-1-lexical-rule-based-preprocessing","title":"Stage 1: Lexical Rule-based Preprocessing","text":""},{"location":"writing/transaction-tagging-ml/#data-normalization","title":"Data Normalization","text":"<p>The first stage handles the chaotic nature of transaction data:</p> <pre><code>def preprocess_transaction(transaction):\n    # Normalize merchant names\n    merchant = normalize_merchant_name(transaction.description)\n\n    # Extract key features\n    features = {\n        'amount': transaction.amount,\n        'timestamp': transaction.timestamp,\n        'narration': clean_narration(transaction.description),\n        'merchant': merchant,\n        'location': extract_location(transaction.description)\n    }\n\n    return features\n</code></pre>"},{"location":"writing/transaction-tagging-ml/#rule-based-classification","title":"Rule-based Classification","text":"<p>Initial categorization using deterministic rules:</p> <pre><code>class LexicalRuleEngine:\n    def __init__(self):\n        self.rules = {\n            'grocery': ['walmart', 'target', 'whole foods'],\n            'gas': ['shell', 'exxon', 'chevron'],\n            'dining': ['restaurant', 'cafe', 'pizza']\n        }\n\n    def apply_rules(self, transaction):\n        merchant_lower = transaction['merchant'].lower()\n\n        for category, keywords in self.rules.items():\n            if any(keyword in merchant_lower for keyword in keywords):\n                return category\n\n        return 'unknown'\n</code></pre>"},{"location":"writing/transaction-tagging-ml/#stage-2-intelligent-f1-tagging","title":"Stage 2: Intelligent F1 Tagging","text":""},{"location":"writing/transaction-tagging-ml/#learning-from-user-behavior","title":"Learning from User Behavior","text":"<p>The F1 tagging system learns from past user tagging patterns:</p> <pre><code>sequenceDiagram\n    participant U as User\n    participant S as System\n    participant DB as Vector DB\n\n    U-&gt;&gt;S: Tags Transaction\n    S-&gt;&gt;S: Generate Embedding\n    S-&gt;&gt;DB: Store Tagged Example\n    DB--&gt;&gt;S: Success\n\n    Note over S: Learning Phase Complete\n\n    U-&gt;&gt;S: New Transaction\n    S-&gt;&gt;DB: Query Similar Transactions\n    DB--&gt;&gt;S: Top-K Matches\n    S-&gt;&gt;S: Calculate F1 Score\n    S-&gt;&gt;U: Suggested Tag</code></pre>"},{"location":"writing/transaction-tagging-ml/#pattern-recognition-algorithm","title":"Pattern Recognition Algorithm","text":"<pre><code>class F1TaggingEngine:\n    def __init__(self, vector_db):\n        self.vector_db = vector_db\n        self.embedding_model = load_embedding_model()\n\n    def learn_from_tagging(self, transaction, user_tag):\n        # Generate embedding for transaction\n        embedding = self.embedding_model.encode(\n            f\"{transaction['merchant']} {transaction['narration']}\"\n        )\n\n        # Store in vector database\n        self.vector_db.upsert(\n            vectors=[{\n                'id': transaction['id'],\n                'values': embedding,\n                'metadata': {\n                    'category': user_tag,\n                    'merchant': transaction['merchant'],\n                    'user_id': transaction['user_id']\n                }\n            }]\n        )\n\n    def suggest_tag(self, transaction, k=5):\n        # Query similar transactions\n        embedding = self.embedding_model.encode(\n            f\"{transaction['merchant']} {transaction['narration']}\"\n        )\n\n        similar_transactions = self.vector_db.query(\n            vector=embedding,\n            top_k=k,\n            filter={'user_id': transaction['user_id']}\n        )\n\n        return self.calculate_f1_optimal_tag(similar_transactions)\n</code></pre>"},{"location":"writing/transaction-tagging-ml/#stage-3-ml-based-categorization","title":"Stage 3: ML-based Categorization","text":""},{"location":"writing/transaction-tagging-ml/#distilbert-architecture","title":"DistilBERT Architecture","text":"<p>The final stage uses a fine-tuned DistilBERT model for semantic understanding:</p> <pre><code>from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n\nclass TransactionCategorizer:\n    def __init__(self):\n        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n        self.model = DistilBertForSequenceClassification.from_pretrained(\n            './fine-tuned-transaction-model'\n        )\n\n    def categorize(self, transaction):\n        # Prepare input text\n        input_text = f\"{transaction['merchant']} {transaction['narration']}\"\n\n        # Tokenize and predict\n        inputs = self.tokenizer(\n            input_text,\n            return_tensors='pt',\n            max_length=128,\n            truncation=True,\n            padding=True\n        )\n\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n\n        return self.decode_prediction(predictions)\n</code></pre>"},{"location":"writing/transaction-tagging-ml/#model-training-pipeline","title":"Model Training Pipeline","text":"<pre><code>graph LR\n    A[Transaction Data] --&gt; B[Data Preprocessing]\n    B --&gt; C[Feature Engineering]\n    C --&gt; D[Train/Val Split]\n    D --&gt; E[DistilBERT Fine-tuning]\n    E --&gt; F[Model Evaluation]\n    F --&gt; G[Production Deployment]\n\n    subgraph \"Training Process\"\n        E1[Tokenization] --&gt; E2[Attention Mechanism]\n        E2 --&gt; E3[Classification Head]\n    end</code></pre>"},{"location":"writing/transaction-tagging-ml/#scaling-to-10m-transactions","title":"Scaling to 10M+ Transactions","text":""},{"location":"writing/transaction-tagging-ml/#performance-optimizations","title":"Performance Optimizations","text":"<pre><code>class ScalableTransactionProcessor:\n    def __init__(self):\n        self.batch_size = 1000\n        self.max_workers = 8\n\n    def process_transactions_batch(self, transactions):\n        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n            # Process in parallel batches\n            futures = []\n            for batch in self.chunk_transactions(transactions, self.batch_size):\n                future = executor.submit(self.process_batch, batch)\n                futures.append(future)\n\n            results = [future.result() for future in futures]\n\n        return self.merge_results(results)\n\n    def chunk_transactions(self, transactions, chunk_size):\n        for i in range(0, len(transactions), chunk_size):\n            yield transactions[i:i + chunk_size]\n</code></pre>"},{"location":"writing/transaction-tagging-ml/#infrastructure-architecture","title":"Infrastructure Architecture","text":"<pre><code>graph TB\n    A[Transaction Stream] --&gt; B[Load Balancer]\n    B --&gt; C[Processing Nodes]\n    C --&gt; D[Vector Database]\n    C --&gt; E[ML Model Cluster]\n\n    subgraph \"Scaling Components\"\n        F[Redis Cache] --&gt; G[Result Storage]\n        H[Monitoring] --&gt; I[Auto-scaling]\n    end\n\n    D --&gt; F\n    E --&gt; F</code></pre>"},{"location":"writing/transaction-tagging-ml/#data-preprocessing-challenges","title":"Data Preprocessing Challenges","text":""},{"location":"writing/transaction-tagging-ml/#handling-merchant-name-variations","title":"Handling Merchant Name Variations","text":"<pre><code>def normalize_merchant_name(description):\n    # Remove common prefixes/suffixes\n    cleaned = re.sub(r'^(POS|ATM|DDA|DEBIT)\\s+', '', description)\n\n    # Standardize location information\n    cleaned = re.sub(r'\\s+[A-Z]{2}\\s+\\d{5}$', '', cleaned)\n\n    # Remove transaction IDs\n    cleaned = re.sub(r'#\\d+', '', cleaned)\n\n    return cleaned.strip().upper()\n</code></pre>"},{"location":"writing/transaction-tagging-ml/#feature-engineering","title":"Feature Engineering","text":"<pre><code>def extract_transaction_features(transaction):\n    features = {\n        # Temporal features\n        'hour': transaction.timestamp.hour,\n        'day_of_week': transaction.timestamp.weekday(),\n        'is_weekend': transaction.timestamp.weekday() &gt;= 5,\n\n        # Amount features\n        'amount_log': np.log1p(abs(transaction.amount)),\n        'is_large_amount': abs(transaction.amount) &gt; 100,\n\n        # Text features\n        'description_length': len(transaction.description),\n        'has_numbers': bool(re.search(r'\\d', transaction.description)),\n        'merchant_category': get_merchant_category(transaction.merchant)\n    }\n\n    return features\n</code></pre>"},{"location":"writing/transaction-tagging-ml/#performance-metrics-and-results","title":"Performance Metrics and Results","text":""},{"location":"writing/transaction-tagging-ml/#system-performance","title":"System Performance","text":"<ul> <li>Throughput: 10M+ transactions processed daily</li> <li>Latency: Sub-100ms categorization per transaction</li> <li>Accuracy: 89% automatic categorization accuracy</li> <li>F1 Score: 0.87 for learning-based suggestions</li> </ul>"},{"location":"writing/transaction-tagging-ml/#model-evaluation","title":"Model Evaluation","text":"<pre><code>def evaluate_model_performance(predictions, ground_truth):\n    metrics = {\n        'accuracy': accuracy_score(ground_truth, predictions),\n        'precision': precision_score(ground_truth, predictions, average='weighted'),\n        'recall': recall_score(ground_truth, predictions, average='weighted'),\n        'f1': f1_score(ground_truth, predictions, average='weighted')\n    }\n\n    return metrics\n</code></pre> <p>This article details the technical implementation of Fold's transaction tagging system. Read the original blog post for more business context.</p>"},{"location":"writing/archive/2024/","title":"2024 Archive","text":"<p>All articles published in 2024.</p>"},{"location":"writing/archive/2024/#november-2024","title":"November 2024","text":"<ul> <li>Building Intelligent Search Systems with LLM-Gym Architecture - 6 min read</li> </ul>"},{"location":"writing/archive/2024/#december-2024","title":"December 2024","text":"<ul> <li>Scaling Transaction Tagging: From Rules to ML at 10M+ Transactions - 10 min read</li> </ul> <p>\u2190 Back to Writing</p>"},{"location":"writing/category/ai-engineering/","title":"AI Engineering","text":"<p>Technical deep-dives into AI system architecture and implementation patterns.</p>"},{"location":"writing/category/ai-engineering/#building-hybrid-scalable-search-systems-with-vector-databases","title":"Building Hybrid Scalable Search Systems with Vector Databases","text":"<p>March 2025 \u2022 8 min read</p> <p>As AI-powered applications become more prevalent, the need for scalable search systems has never been greater. This article explores the architecture patterns and best practices for building search systems that can handle millions of users while maintaining sub-second response times.</p> <p>Vector databases are specialized storage systems optimized for high-dimensional vector operations. They enable similarity search at scale, which is crucial for modern AI applications. The key to building scalable systems lies in understanding the trade-offs between accuracy, speed, and resource consumption.</p> <p>Continue reading \u2192</p>"},{"location":"writing/category/ai-engineering/#scaling-transaction-tagging-from-rules-to-ml-at-10m-transactions","title":"Scaling Transaction Tagging: From Rules to ML at 10M+ Transactions","text":"<p>December 2024 \u2022 10 min read</p> <p>Building intelligent financial systems requires solving complex data challenges at scale. This article explores the architecture and implementation of an automated transaction tagging system that processes over 10 million transactions, transitioning from rule-based approaches to sophisticated ML pipelines.</p> <p>The system implements a three-stage approach: lexical rule-based preprocessing, intelligent F1 tagging that learns from user behavior, and ML-based categorization using a fine-tuned DistilBERT model for semantic understanding.</p> <p>Continue reading \u2192</p>"},{"location":"writing/category/enterprise-applications/","title":"Enterprise Applications","text":"<p>Insights about implementing AI in business environments and enterprise workflows.</p>"},{"location":"writing/category/enterprise-applications/#building-intelligent-search-systems-with-llm-gym-architecture","title":"Building Intelligent Search Systems with LLM-Gym Architecture","text":"<p>November 2024 \u2022 6 min read</p> <p>Modern knowledge management requires more than traditional search - it needs intelligent systems that can understand context, semantics, and user intent. This article explores the architecture of LLM-Gym, demonstrating how to build sophisticated search and chat systems over curated content.</p> <p>The system implements a three-layer orchestration pattern combining hybrid search strategies, multi-database architecture, and intelligent context management. Key innovations include specialized database usage and reciprocal rank fusion for optimal search results.</p> <p>Continue reading \u2192</p>"},{"location":"writing/category/enterprise-applications/#scaling-transaction-tagging-from-rules-to-ml-at-10m-transactions","title":"Scaling Transaction Tagging: From Rules to ML at 10M+ Transactions","text":"<p>December 2024 \u2022 10 min read</p> <p>Building intelligent financial systems requires solving complex data challenges at scale. This article explores the architecture and implementation of an automated transaction tagging system that processes over 10 million transactions, transitioning from rule-based approaches to sophisticated ML pipelines.</p> <p>The enterprise-scale system demonstrates how to handle unstructured financial data, implement learning algorithms that adapt to user behavior, and deploy ML models capable of processing millions of transactions daily while maintaining high accuracy and sub-100ms response times.</p> <p>Continue reading \u2192</p>"},{"location":"writing/category/search-retrieval/","title":"Search &amp; Retrieval","text":"<p>Articles about search technologies, vector databases, and information retrieval systems.</p>"},{"location":"writing/category/search-retrieval/#building-scalable-search-systems-with-vector-databases","title":"Building Scalable Search Systems with Vector Databases","text":"<p>March 2025 \u2022 8 min read</p> <p>As AI-powered applications become more prevalent, the need for scalable search systems has never been greater. This article explores the architecture patterns and best practices for building search systems that can handle millions of users while maintaining sub-second response times.</p> <p>Vector databases are specialized storage systems optimized for high-dimensional vector operations. They enable similarity search at scale, which is crucial for modern AI applications. Key concepts include horizontal partitioning, intelligent caching strategies, and efficient load balancing techniques.</p> <p>Continue reading \u2192</p>"}]}